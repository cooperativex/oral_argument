# Get project root directory
import os
# This file is at: src/evaluators/.../generate/*.py
# Need to go up 5 levels to reach project root
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))))

import json
import re
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import Dict, List, Tuple, Optional
import time
from datetime import datetime
import sys
import warnings
warnings.filterwarnings('ignore')

# Fix Windows encoding issue
if sys.platform == "win32":
    import codecs
    sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())

class JusticeKeywordEvaluator:
    def __init__(self):
        # Qwen2.5-72B-Instruct local model configuration
        model_path = os.path.join(project_root, "qwen_models", "Qwen_Qwen2.5-72B-Instruct")
        self.model_name = model_path

        print("ğŸ”„ Loading Qwen2.5-72B-Instruct model...")
        print(f"   Model path: {model_path}")

        # Load tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True
        )

        # Check if CUDA is available
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   Using device: {self.device}")

        # Load model with appropriate settings
        if self.device == "cuda":
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16,  # Use half precision for GPU
                device_map="auto",
                trust_remote_code=True
            )
        else:
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float32,  # Use full precision for CPU
                trust_remote_code=True
            )
            self.model = self.model.to(self.device)

        self.model.eval()  # Set to evaluation mode
        print("âœ… Model loaded successfully!")

        self.results = {
            "evaluation_timestamp": datetime.now().isoformat(),
            "total_questions": 0,
            "correct_answers": 0,
            "accuracy": 0.0,
            "detailed_results": [],
            "task_type": "justice_keyword_identification"
        }
        self.dialogue_summaries = {}
        self.context_patterns = {}


    def count_tokens(self, text: str) -> int:
        """Count tokens in text using the actual tokenizer"""
        return len(self.tokenizer.encode(text, add_special_tokens=False))

    def reduce_conversations_to_fit_tokens(self, context: str, max_tokens: int, use_cumulative_context: bool = True) -> str:
        """Reduce number of conversations instead of hard truncation to fit token limit"""
        current_tokens = self.count_tokens(context)

        if current_tokens <= max_tokens:
            return context

        print(f"âš ï¸ Context too long ({current_tokens} tokens), reducing conversations to fit {max_tokens} tokens")

        # If no previous conversations, fall back to smart truncation
        if "Previous Conversations" not in context or not use_cumulative_context:
            return self.truncate_context_smartly(context, max_tokens)

        # Split context into background and conversation parts
        parts = context.split("\\n\\nPrevious Conversations")
        base_part = parts[0]  # Background context
        base_tokens = self.count_tokens(base_part)

        # If base part is already too long, truncate it first
        if base_tokens > max_tokens - 500:  # Reserve 500 tokens for at least some conversation
            print(f"    âš ï¸ Background context itself is too long ({base_tokens} tokens), truncating background first")
            base_part = self.truncate_context_smartly(base_part, max_tokens - 500)
            base_tokens = self.count_tokens(base_part)

        if len(parts) > 1:
            # We have previous conversations, reduce them gradually
            remaining_tokens = max_tokens - base_tokens - 200  # Reserve for formatting

            # Parse conversation section
            prev_conv_section = parts[1]
            conversations_text = prev_conv_section.split("\\n\\n---\\n\\n")

            # Start with most recent conversations and add until we hit limit
            conversations_to_keep = []
            current_conv_tokens = 0

            # conversations_text[0] contains the header, conversations_text[1:] contain actual conversations
            if len(conversations_text) > 1:
                for conv_text in reversed(conversations_text[1:]):  # Most recent first
                    conv_tokens = self.count_tokens(conv_text)
                    if current_conv_tokens + conv_tokens <= remaining_tokens:
                        conversations_to_keep.insert(0, conv_text)  # Insert at beginning for chronological order
                        current_conv_tokens += conv_tokens
                        print(f"    âœ“ Keeping conversation ({conv_tokens} tokens, total: {current_conv_tokens})")
                    else:
                        print(f"    âœ— Dropping conversation ({conv_tokens} tokens, would exceed limit)")

            # Rebuild context with reduced conversations
            if conversations_to_keep:
                result = base_part + f"\\n\\nPrevious Conversations (Last {len(conversations_to_keep)}, reduced to fit tokens):\\n" + "\\n\\n---\\n\\n".join(conversations_to_keep)
            else:
                result = base_part  # No previous conversations fit, use only background
                print(f"    âš ï¸ No previous conversations fit in token budget, using background only")
        else:
            result = base_part

        final_tokens = self.count_tokens(result)
        print(f"âœ‚ï¸ Conversation reduction complete: {current_tokens} -> {final_tokens} tokens")
        return result

    def truncate_context_smartly(self, context: str, max_tokens: int) -> str:
        """Intelligently truncate context to fit within token limit - used as fallback when conversation reduction is not applicable"""
        current_tokens = self.count_tokens(context)

        if current_tokens <= max_tokens:
            return context

        print(f"âš ï¸ Applying fallback smart truncation ({current_tokens} tokens) -> {max_tokens} tokens")

        # Split by sections and keep the most important parts
        sections = context.split('\n\n')

        # Always keep background if it exists
        background_parts = []
        cumulative_parts = []
        current_parts = []

        for section in sections:
            if 'Background Context:' in section or 'Case Background' in section:
                background_parts.append(section)
            elif 'Previous Conversations' in section or 'Previous Dialogue' in section:
                cumulative_parts.append(section)
            elif 'Current Conversation' in section:
                current_parts.append(section)
            else:
                background_parts.append(section)

        # Prioritize: Current > Background > Previous (most recent first)
        final_parts = []
        tokens_used = 0

        # Add current conversation context first (most important)
        for part in current_parts:
            part_tokens = self.count_tokens(part)
            if tokens_used + part_tokens <= max_tokens:
                final_parts.append(part)
                tokens_used += part_tokens

        # Add background context
        for part in background_parts:
            part_tokens = self.count_tokens(part)
            if tokens_used + part_tokens <= max_tokens:
                final_parts.append(part)
                tokens_used += part_tokens
            else:
                # Truncate this part
                remaining_tokens = max_tokens - tokens_used
                if remaining_tokens > 100:  # Only add if meaningful space left
                    truncated_part = part[:remaining_tokens * 4]  # Rough char to token ratio
                    actual_tokens = self.count_tokens(truncated_part)
                    if tokens_used + actual_tokens <= max_tokens:
                        final_parts.append(truncated_part + "...")
                        tokens_used += actual_tokens
                break

        # Add previous conversations (most recent first, but only if space)
        cumulative_parts.reverse()  # Most recent first
        for part in cumulative_parts:
            part_tokens = self.count_tokens(part)
            if tokens_used + part_tokens <= max_tokens:
                final_parts.append(part)
                tokens_used += part_tokens
            else:
                # Try to fit a truncated version
                remaining_tokens = max_tokens - tokens_used
                if remaining_tokens > 200:  # Only if significant space
                    truncated_part = part[:remaining_tokens * 4]
                    actual_tokens = self.count_tokens(truncated_part)
                    if tokens_used + actual_tokens <= max_tokens:
                        final_parts.append(truncated_part + "...")
                        tokens_used += actual_tokens
                break

        result = '\n\n'.join(final_parts)
        final_tokens = self.count_tokens(result)
        print(f"âœ‚ï¸ Smart truncation complete: {current_tokens} -> {final_tokens} tokens")
        return result

    def generate_response(self, messages: List[Dict], max_tokens: int = 300):
        """Generate response using local Qwen2.5-72B model"""
        try:
            print(f"ğŸ”§ Starting generation with Qwen2.5-72B, max_tokens={max_tokens}")

            # Format messages using Qwen's chat template
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            # Tokenize and generate
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.device)

            with torch.no_grad():
                generated_ids = self.model.generate(
                    model_inputs.input_ids,
                    max_new_tokens=max_tokens,
                    temperature=0.1,
                    top_p=0.8,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )

            # Extract only the generated part (remove input prompt)
            generated_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]

            ai_response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            print(f"ğŸ“„ Raw response: {repr(ai_response)}")

            if ai_response is None:
                ai_response = ""
            else:
                ai_response = ai_response.strip()

            print(f"ğŸ“„ Generated response length: {len(ai_response)} characters")
            print("-" * 50)

            return ai_response

        except Exception as e:
            print(f"âŒ Generation error: {str(e)}")
            import traceback
            traceback.print_exc()
            return f"Error generating response: {str(e)}"


    def summarize_dialogue(self, dialogue: str, max_chars: int = 4000) -> str:
        """
        Generate concise summary of dialogue using Qwen2.5
        """
        try:
            if len(dialogue) <= max_chars:
                return dialogue

            # If dialogue is short, return directly
            if len(dialogue) <= 5000:
                return dialogue[:max_chars] + "..."

            messages = [
                {
                    'role': 'system',
                    'content': (
                        "You are a legal dialogue summary expert. Please summarize the given court dialogue into a concise summary, "
                        f"limited to {max_chars} characters. Preserve key legal issues, main concerns of judges, "
                        "and core arguments of lawyers. Use objective, accurate language."
                    )
                },
                {
                    'role': 'user',
                    'content': f"Please summarize the following court dialogue (limit {max_chars} characters):\n\n{dialogue}"
                }
            ]

            summary = self.generate_response(messages, max_tokens=2000)

            # Ensure summary doesn't exceed character limit
            if len(summary) > max_chars:
                summary = summary[:max_chars] + "..."

            return summary.strip()

        except Exception as e:
            print(f"Error generating summary: {e}")
            # If summary fails, use simple truncation
            return dialogue[:max_chars] + "..." if len(dialogue) > max_chars else dialogue

    def get_qwen_answer(self, messages: List[Dict]) -> str:
        """Get Qwen2.5's answer to the question"""
        try:
            response = self.generate_response(messages, max_tokens=3072)
            answer = response.strip().upper()
            import re
            print(f"ğŸ” Raw answer from model: {repr(response)}")

            # Extract just the letter if there's extra text
            if len(answer) > 1:
                """
                ä» LLM è¾“å‡ºé‡Œæå– \boxed{...} æ ¼å¼çš„ç­”æ¡ˆ
                """
                match = re.search(r"\\boxed\{(.*?)\}", response)
                if match:
                    return match.group(1).strip()

                # If not in box, extract first uppercase letter
                first_letter_match = re.search(r'[ABCD]', response)
                if first_letter_match:
                    extracted_letter = first_letter_match.group(0)
                    print(f"âœ… Extracted first letter: {extracted_letter}")
                    return extracted_letter

            # If exact match
            if answer in ['A', 'B', 'C', 'D']:
                print(f"âœ… Direct match answer: {answer}")
                return answer

            print(f"âŒ No valid answer found in: {repr(answer)}")
            return 'INVALID'

        except Exception as e:
            print(f"Error getting Qwen2.5 response: {e}")
            return 'ERROR'

    def build_evaluation_prompt_no_leak(self, cumulative_context: str, question_data: Dict) -> List[Dict]:
        """
        Build evaluation prompt for advocate keyword identification
        Uses cumulative context for better understanding
        SECURITY: Does not include original statement to prevent answer leakage
        """
        # Format the options nicely
        options_text = "\n".join([f"{key}: {value}" for key, value in question_data["options"].items()])

        # Get speaker info but NOT the original statement (contains answer)
        context_info = question_data.get('metadata', {}).get('context_info', {})
        speaker = context_info.get('speaker', '')

        # SECURITY: Do not include original_statement as it contains the correct answer

        messages = [
            {
                'role': 'system',
                'content': (
                    "You are an expert in legal language analysis and judicial questioning comprehension. "
                    "You will identify key legal terms, phrases, or concepts from judicial questions.\n\n"

                    "Your task is to:\n"
                    "1. Analyze the case context and dialogue background\n"
                    "2. Focus on the specific judicial question provided\n"
                    "3. Identify which legal concept, term, or phrase is most relevant to the justice's inquiry\n"
                    "4. Consider the legal significance and context of each option\n"
                    "5. Select the option that best captures the key legal point being made\n\n"

                    "Focus on:\n"
                    "- Core legal concepts and principles\n"
                    "- Important statutory references and legal authorities\n"
                    "- Procedural requirements and legal standards\n"
                    "- Case law principles and precedents\n"
                    "- Constitutional provisions and interpretations\n"
                    "- Technical legal terminology essential to the argument\n\n"

                    "Provide brief one short sentence analysis and answer by selecting the options only"
                    "Please reiterate your answer, with your final answer a single answer of the form \\boxed{{answer}} at the end of your response."
                )
            },
            {
                'role': 'user',
                'content': (
                    f"Case Context:\n{cumulative_context}\n\n"
                    f"Question: {question_data['question']}\n\n"
                    f"Options:\n{options_text}\n\n"
                    f"Based on the context provided, what is your answer?"
                )
            }
        ]
        return messages

    def evaluate_dataset(self, input_file: str, output_file: str = None, delay: float = 1.0,
                        model: str = "qwen2.5-72b-instruct", use_cumulative_context: bool = True,
                        analyze_patterns: bool = True, include_current_conversation: bool = True,
                        use_summary: bool = False):
        """
        Evaluate the advocate keyword identification dataset
        """

        # Load the generated questions
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as e:
            print(f"Error loading input file: {e}")
            return

        base_background = data.get('background', '')
        conversations = data.get('conversations', [])

        print(f"å¼€å§‹ä½¿ç”¨{model}è¯„ä¼°{len(conversations)}ä¸ªå¯¹è¯...")
        print(f"åŸºç¡€èƒŒæ™¯é•¿åº¦: {len(base_background)} å­—ç¬¦")
        print(f"ä½¿ç”¨ç´¯ç§¯ä¸Šä¸‹æ–‡: {use_cumulative_context}")
        print(f"åŒ…å«å½“å‰å¯¹è¯: {include_current_conversation}")
        print(f"ä½¿ç”¨æ‘˜è¦å‹ç¼©: {use_summary}")
        print(f"ä»»åŠ¡ç±»å‹: æ³•å®˜å…³é”®è¯è¯†åˆ«")
        print("-" * 60)

        # Sort conversations by ID to ensure proper order
        conversations.sort(key=lambda x: x.get('conversation_id', 0))

        # Process each conversation
        for conv in conversations:
            conv_id = conv.get('conversation_id', 0)
            dialogue = conv.get('dialogue', '')
            target_dialogue = conv.get('target_dialogue', dialogue)
            questions = conv.get('questions', [])

            print(f"\nå¤„ç†å¯¹è¯ {conv_id}:")
            print(f"  é—®é¢˜æ•°: {len(questions)}")

            # Process each question in the conversation
            for q_idx, question_data in enumerate(questions):
                if 'question' not in question_data or 'options' not in question_data:
                    print(f"    è·³è¿‡æ ¼å¼é”™è¯¯çš„é—®é¢˜ {q_idx}")
                    continue

                # Build context with background and cumulative conversations
                context = ""
                if base_background:
                    if len(base_background) > 4000:
                        context = base_background[:4000] + "..."
                    else:
                        context = base_background

                # Add cumulative context (previous conversations) if enabled
                # SECURITY: These are complete previous conversations, should not contain current question's answer
                if use_cumulative_context:
                    current_conv_id = conv.get('conversation_id', 0)
                    previous_conversations_all = [c for c in conversations if c.get('conversation_id', 0) < current_conv_id]

                    # Sort by conversation_id to maintain order and take last 3
                    previous_conversations_all.sort(key=lambda x: x.get('conversation_id', 0))
                    previous_conversations_to_include = previous_conversations_all[-3:] if len(previous_conversations_all) > 3 else previous_conversations_all

                    if previous_conversations_to_include:
                        previous_dialogues = []
                        correct_phrase = question_data.get('metadata', {}).get('correct_phrase', '')

                        for prev_conv in previous_conversations_to_include:
                            prev_dialogue = prev_conv.get('dialogue', '').strip()
                            if prev_dialogue:
                                # Security check: ensure the current question's answer is not in previous conversations
                                # Use more sophisticated matching - only flag if phrase appears as complete words
                                if correct_phrase and len(correct_phrase) > 3:  # Only check phrases longer than 3 chars
                                    # Create word boundary pattern to avoid partial matches
                                    pattern = r'\b' + re.escape(correct_phrase.lower()) + r'\b'
                                    if re.search(pattern, prev_dialogue.lower()):
                                        print(f"    âš ï¸ å®‰å…¨è­¦å‘Š: å‰é¢å¯¹è¯ {prev_conv.get('conversation_id', 0)} åŒ…å«å½“å‰é—®é¢˜çš„ç­”æ¡ˆï¼Œè·³è¿‡")
                                        continue
                                previous_dialogues.append(prev_dialogue)

                        if previous_dialogues:
                            context += f"\n\nå‰é¢çš„å¯¹è¯ (æœ€å {len(previous_dialogues)} ä¸ª, å·²éªŒè¯æ— ç­”æ¡ˆæ³„éœ²):\n" + "\n\n---\n\n".join(previous_dialogues)
                            print(f"    âœ… å®‰å…¨æ·»åŠ äº†ç´¯ç§¯ä¸Šä¸‹æ–‡ ({len(previous_dialogues)} ä¸ªå‰é¢çš„å¯¹è¯, å·²è¿‡æ»¤ç­”æ¡ˆæ³„éœ²)")

                # Add current conversation context BEFORE the target statement (NO ANSWER LEAKAGE)
                if include_current_conversation:
                    dialogue_content = conv.get('dialogue', '')
                    if dialogue_content:
                        # Get the original statement that contains the correct answer
                        context_info = question_data.get('metadata', {}).get('context_info', {})
                        original_statement = context_info.get('original_statement', '')

                        # ğŸ” DEBUG: Check data structure
                        print(f"    ğŸ” Debug - metadata keys: {list(question_data.get('metadata', {}).keys())}")
                        print(f"    ğŸ” Debug - context_info keys: {list(context_info.keys())}")
                        print(f"    ğŸ” Debug - original_statement exists: {bool(original_statement)}")
                        if original_statement:
                            print(f"    ğŸ” Debug - original_statement length: {len(original_statement)} chars")
                            print(f"    ğŸ” Debug - dialogue_content length: {len(dialogue_content)} chars")

                        if original_statement:
                            # ğŸ”ª PRECISE CUTOFF: Use complete original_statement for exact matching
                            print(f"    ğŸ” å°è¯•ç²¾ç¡®åŒ¹é…å®Œæ•´åŸå§‹é™ˆè¿°...")
                            cutoff_position = dialogue_content.find(original_statement)

                            if cutoff_position != -1:
                                print(f"    ğŸ¯ âœ… æ‰¾åˆ°å®Œæ•´åŸå§‹é™ˆè¿°åœ¨ä½ç½® {cutoff_position}")
                                print(f"    ğŸ“ åŸå§‹é™ˆè¿°å¼€å¤´: '{original_statement[:50]}...'")
                                print(f"    ğŸ“ åŒ¹é…å¤„çš„å¯¹è¯: '{dialogue_content[cutoff_position:cutoff_position+50]}...'")
                            else:
                                print(f"    âŒ å®Œæ•´åŸå§‹é™ˆè¿°æœªæ‰¾åˆ°ï¼Œå°è¯•éƒ¨åˆ†åŒ¹é…...")
                                # Fallback: try to find substantial part of the statement
                                original_words = original_statement.split()
                                if len(original_words) >= 10:
                                    # Try first 10 words as a fallback
                                    partial_statement = ' '.join(original_words[:10])
                                    print(f"    ğŸ” å°è¯•åŒ¹é…å‰10ä¸ªè¯: '{partial_statement}'")
                                    cutoff_position = dialogue_content.find(partial_statement)
                                    if cutoff_position != -1:
                                        print(f"    ğŸ¯ âœ… æ‰¾åˆ°éƒ¨åˆ†åŸå§‹é™ˆè¿°åœ¨ä½ç½® {cutoff_position} (å‰10ä¸ªè¯)")
                                        print(f"    ğŸ“ åŒ¹é…å¤„çš„å¯¹è¯: '{dialogue_content[cutoff_position:cutoff_position+50]}...'")
                                    else:
                                        print(f"    âŒ æ— æ³•åœ¨å¯¹è¯ä¸­æ‰¾åˆ°åŸå§‹é™ˆè¿°çš„ä»»ä½•éƒ¨åˆ†")
                                        print(f"    ğŸ“ å¯»æ‰¾çš„é™ˆè¿°: '{original_statement[:100]}...'")
                                        print(f"    ğŸ“ å¯¹è¯å¼€å¤´: '{dialogue_content[:200]}...'")
                                        print(f"    ğŸ“ å¯¹è¯ç»“å°¾: '...{dialogue_content[-200:]}'" if len(dialogue_content) > 200 else "")
                                elif len(original_words) >= 5:
                                    # Try first 5 words for very short statements
                                    partial_statement = ' '.join(original_words[:5])
                                    print(f"    ğŸ” å°è¯•åŒ¹é…å‰5ä¸ªè¯: '{partial_statement}'")
                                    cutoff_position = dialogue_content.find(partial_statement)
                                    if cutoff_position != -1:
                                        print(f"    ğŸ¯ âœ… æ‰¾åˆ°éƒ¨åˆ†åŸå§‹é™ˆè¿°åœ¨ä½ç½® {cutoff_position} (å‰5ä¸ªè¯)")
                                    else:
                                        print(f"    âŒ å‰5ä¸ªè¯ä¹Ÿæœªæ‰¾åˆ°")
                                else:
                                    print(f"    âŒ åŸå§‹é™ˆè¿°å¤ªçŸ­({len(original_words)} è¯)ï¼Œæ— æ³•å¯é åŒ¹é…")

                            if cutoff_position > 0:  # Ensure there's some content before the cutoff
                                # ğŸ”ª HARD CUT: Include only dialogue BEFORE original_statement starts
                                safe_dialogue = dialogue_content[:cutoff_position].strip()

                                print(f"    ğŸ”ª æˆªæ–­ç»“æœéªŒè¯:")
                                print(f"    ğŸ“ åŸå§‹å¯¹è¯é•¿åº¦: {len(dialogue_content)} å­—ç¬¦")
                                print(f"    ğŸ“ æˆªæ–­ä½ç½®: {cutoff_position}")
                                print(f"    ğŸ“ æˆªæ–­åé•¿åº¦: {len(safe_dialogue)} å­—ç¬¦")
                                print(f"    ğŸ“ æˆªæ–­åå¯¹è¯ç»“å°¾: '...{safe_dialogue[-100:]}'" if len(safe_dialogue) > 100 else f"    ğŸ“ æˆªæ–­åå®Œæ•´å¯¹è¯: '{safe_dialogue}'")

                                # Verify that original_statement is NOT in the safe_dialogue
                                if original_statement in safe_dialogue:
                                    print(f"    ğŸš¨ è­¦å‘Š: original_statementä»ç„¶å­˜åœ¨äºæˆªæ–­åçš„å¯¹è¯ä¸­ï¼")
                                else:
                                    print(f"    âœ… éªŒè¯é€šè¿‡: original_statementä¸åœ¨æˆªæ–­åçš„å¯¹è¯ä¸­")

                                if safe_dialogue and len(safe_dialogue) > 50:  # Ensure meaningful content
                                    # Since we cut at the exact start of original_statement, no additional checks needed
                                    context += f"\n\nå½“å‰å¯¹è¯ (åœ¨original_statementä¹‹å‰æˆªæ–­):\n{safe_dialogue}"
                                    print(f"    âœ… ç¡¬æˆªæ–­æˆåŠŸ: åœ¨original_statementå¼€å§‹ä½ç½® {cutoff_position} å¤„æˆªæ–­")
                                    print(f"    ğŸ“ æœ€ç»ˆå®‰å…¨å¯¹è¯é•¿åº¦: {len(safe_dialogue)} å­—ç¬¦")
                                else:
                                    print(f"    âš ï¸ æˆªæ–­åçš„ä¸Šæ–‡å¤ªçŸ­ ({len(safe_dialogue) if safe_dialogue else 0} å­—ç¬¦), è·³è¿‡")
                            elif cutoff_position == 0:
                                print(f"    âš ï¸ original_statementåœ¨å¯¹è¯å¼€å¤´ (ä½ç½®0)ï¼Œæ²¡æœ‰å¯ç”¨çš„ä¸Šæ–‡")
                            else:
                                print(f"    âŒ æœªæ‰¾åˆ°original_statementï¼Œæ— æ³•æˆªæ–­")
                        else:
                            print(f"    âš ï¸ ç¼ºå°‘original_statementï¼Œè·³è¿‡å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡ä»¥ç¡®ä¿å®‰å…¨")
                    else:
                        print(f"    è­¦å‘Š: å½“å‰å¯¹è¯å†…å®¹ä¸ºç©º")

                # Final context validation
                context_tokens = self.count_tokens(context)
                max_allowed_context_tokens = 100000  # Very large limit for 72B model

                if context_tokens > max_allowed_context_tokens:
                    print(f"    âš ï¸ ä¸Šä¸‹æ–‡è¿‡é•¿ ({context_tokens} tokens)ï¼Œåº”ç”¨å¯¹è¯å‡å°‘")
                    context = self.reduce_conversations_to_fit_tokens(context, max_allowed_context_tokens, use_cumulative_context)
                    context_tokens = self.count_tokens(context)

                context_length = len(context)
                print(f"  é—®é¢˜ {q_idx + 1}/{len(questions)}")
                print(f"    ä¸Šä¸‹æ–‡é•¿åº¦: {context_length} å­—ç¬¦, {context_tokens} tokens")

                # Build evaluation prompt
                messages = self.build_evaluation_prompt_no_leak(context, question_data)

                # Final security check: ensure no answer leakage in CONTEXT (not in question itself)
                # For fill-in-blank questions, the answer phrase may legitimately appear in the question text
                correct_phrase = question_data.get('metadata', {}).get('correct_phrase', '')
                if correct_phrase and len(correct_phrase) > 3:  # Only check phrases longer than 3 chars
                    # Only check the CONTEXT part, not the question itself
                    context_only = context.lower()  # Only check the context we built, not the question
                    pattern = r'\b' + re.escape(correct_phrase.lower()) + r'\b'
                    if re.search(pattern, context_only):
                        print(f"    ğŸš¨ ä¸¥é‡å®‰å…¨è­¦å‘Š: æ£€æµ‹åˆ°ç­”æ¡ˆæ³„éœ²åœ¨ä¸Šä¸‹æ–‡ä¸­! ç­”æ¡ˆ: '{correct_phrase}'")
                        print(f"    è·³è¿‡æ­¤é—®é¢˜ä»¥é˜²æ­¢ç­”æ¡ˆæ³„éœ²")
                        continue
                    else:
                        print(f"    âœ… å®‰å…¨æ£€æŸ¥é€šè¿‡: ç­”æ¡ˆ '{correct_phrase}' ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ (å¯èƒ½åœ¨é—®é¢˜æ–‡æœ¬ä¸­ï¼Œè¿™æ˜¯æ­£å¸¸çš„)")
                else:
                    print(f"    âœ… è·³è¿‡å®‰å…¨æ£€æŸ¥: çŸ­è¯­è¿‡çŸ­æˆ–ä¸å­˜åœ¨")

                qwen_answer = self.get_qwen_answer(messages)

                # Check if answer is correct
                correct_answer = question_data.get('answer', '').upper()
                is_correct = qwen_answer == correct_answer

                # Get metadata for this question
                metadata = question_data.get('metadata', {})
                context_info = metadata.get('context_info', {})

                result = {
                    "conversation_id": conv_id,
                    "question_index": q_idx,
                    "question": question_data['question'],
                    "correct_answer": correct_answer,
                    "qwen_answer": qwen_answer,
                    "is_correct": is_correct,
                    "options": question_data['options'],
                    "model_used": model,
                    "context_length": context_length,
                    "task_type": "justice_keyword_identification",
                    "metadata": {
                        "correct_phrase": metadata.get('correct_phrase', ''),
                        "category": metadata.get('category', ''),
                        "difficulty": metadata.get('difficulty', ''),
                        "word_count": metadata.get('word_count', 0),
                        "speaker": context_info.get('speaker', ''),
                        "explanation": metadata.get('explanation', '')
                    }
                }

                self.results["detailed_results"].append(result)

                if result['is_correct']:
                    self.results["correct_answers"] += 1
                self.results["total_questions"] += 1

                correct_phrase = metadata.get('correct_phrase', '')
                print(f"    ç»“æœ: {'âœ“' if is_correct else 'âœ—'} (é¢„æµ‹: {qwen_answer}, æ­£ç¡®: {correct_answer}) - å…³é”®è¯: {correct_phrase}")

                # Add delay to avoid rate limits
                time.sleep(delay)

        # Calculate final accuracy
        if self.results["total_questions"] > 0:
            self.results["accuracy"] = self.results["correct_answers"] / self.results["total_questions"]

        # Add configuration info to results
        self.results["model_used"] = model
        self.results["used_cumulative_context"] = use_cumulative_context
        self.results["include_current_conversation"] = include_current_conversation
        self.results["use_summary"] = use_summary
        self.results["analyzed_patterns"] = analyze_patterns
        self.results["dialogue_summaries"] = self.dialogue_summaries
        self.results["context_patterns"] = self.context_patterns

        # Print summary
        self.print_summary()

        # Save results
        if output_file:
            self.save_results(output_file)

    def print_summary(self):
        """Print evaluation summary"""
        print("\n" + "="*60)
        print("æ³•å®˜å…³é”®è¯è¯†åˆ«è¯„ä¼°æ‘˜è¦ (Qwen2.5æ¨¡å‹)")
        print("="*60)
        print(f"ä»»åŠ¡ç±»å‹: {self.results.get('task_type', 'æœªçŸ¥')}")
        print(f"ä½¿ç”¨çš„æ¨¡å‹: {self.results.get('model_used', 'æœªçŸ¥')}")
        print(f"ç´¯ç§¯ä¸Šä¸‹æ–‡: {self.results.get('used_cumulative_context', 'æœªçŸ¥')}")
        print(f"åŒ…å«å½“å‰å¯¹è¯: {self.results.get('include_current_conversation', 'æœªçŸ¥')}")
        print(f"ä½¿ç”¨æ‘˜è¦å‹ç¼©: {self.results.get('use_summary', 'æœªçŸ¥')}")
        print(f"æ¨¡å¼åˆ†æ: {self.results.get('analyzed_patterns', 'æœªçŸ¥')}")
        print(f"æ€»é—®é¢˜æ•°: {self.results['total_questions']}")
        print(f"æ­£ç¡®è¯†åˆ«æ•°: {self.results['correct_answers']}")
        print(f"è¯†åˆ«å‡†ç¡®ç‡: {self.results['accuracy']:.2%}")

        # Breakdown by conversation
        conv_stats = {}
        for result in self.results["detailed_results"]:
            conv_id = result["conversation_id"]
            if conv_id not in conv_stats:
                conv_stats[conv_id] = {"correct": 0, "total": 0}
            conv_stats[conv_id]["total"] += 1
            if result["is_correct"]:
                conv_stats[conv_id]["correct"] += 1

        print("\næŒ‰å¯¹è¯åˆ†è§£:")
        for conv_id in sorted(conv_stats.keys()):
            stats = conv_stats[conv_id]
            accuracy = stats["correct"] / stats["total"] if stats["total"] > 0 else 0
            print(f"  å¯¹è¯ {conv_id}: {stats['correct']}/{stats['total']} ({accuracy:.1%})")

        # Context length analysis
        if self.results["detailed_results"]:
            context_lengths = [r.get("context_length", 0) for r in self.results["detailed_results"]]
            avg_context_length = sum(context_lengths) / len(context_lengths)
            print(f"\nå¹³å‡ä¸Šä¸‹æ–‡é•¿åº¦: {avg_context_length:.0f} å­—ç¬¦")

        # Answer distribution analysis
        answer_distribution = {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'ERROR': 0, 'INVALID': 0}
        for result in self.results["detailed_results"]:
            qwen_answer = result["qwen_answer"]
            if qwen_answer in answer_distribution:
                answer_distribution[qwen_answer] += 1

        print(f"\nQwen2.5 ç­”æ¡ˆåˆ†å¸ƒ:")
        for answer, count in answer_distribution.items():
            if count > 0:
                percentage = (count / self.results['total_questions']) * 100
                print(f"  {answer}: {count} ({percentage:.1f}%)")

    def save_results(self, output_file: str):
        """Save evaluation results to JSON file"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False)
            print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            print(f"ä¿å­˜ç»“æœæ—¶å‡ºé”™: {e}")


def save_overall_accuracy_summary(output_dir: str, model_name: str, all_results: dict):
    """Save overall accuracy summary for all files and modes"""
    try:
        summary_file = os.path.join(output_dir, f"{model_name}_overall_accuracy_summary.txt")

        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write(f"æ•´ä½“æ³•å®˜å…³é”®è¯è¯†åˆ«å‡†ç¡®ç‡æ‘˜è¦ ({model_name.upper()}æ¨¡å‹)\n")
            f.write("=" * 80 + "\n")
            f.write(f"è¯„ä¼°æ—¶é—´: {datetime.now().isoformat()}\n")
            f.write(f"è¯„ä¼°æ–‡ä»¶æ•°: {len(all_results)//3}\n")  # æ¯ä¸ªæ–‡ä»¶æœ‰3ä¸ªæ¨¡å¼
            f.write("\n")

            # Calculate overall statistics for each mode
            mode_stats = {
                "mode1": {"total": 0, "correct": 0, "files": []},
                "mode2": {"total": 0, "correct": 0, "files": []},
                "mode3": {"total": 0, "correct": 0, "files": []}
            }

            for result_key, result_data in all_results.items():
                if "mode1" in result_key:
                    mode_stats["mode1"]["total"] += result_data["total_questions"]
                    mode_stats["mode1"]["correct"] += result_data["correct_answers"]
                    mode_stats["mode1"]["files"].append({
                        "file": result_key,
                        "accuracy": result_data["accuracy"],
                        "total": result_data["total_questions"],
                        "correct": result_data["correct_answers"]
                    })
                elif "mode2" in result_key:
                    mode_stats["mode2"]["total"] += result_data["total_questions"]
                    mode_stats["mode2"]["correct"] += result_data["correct_answers"]
                    mode_stats["mode2"]["files"].append({
                        "file": result_key,
                        "accuracy": result_data["accuracy"],
                        "total": result_data["total_questions"],
                        "correct": result_data["correct_answers"]
                    })
                elif "mode3" in result_key:
                    mode_stats["mode3"]["total"] += result_data["total_questions"]
                    mode_stats["mode3"]["correct"] += result_data["correct_answers"]
                    mode_stats["mode3"]["files"].append({
                        "file": result_key,
                        "accuracy": result_data["accuracy"],
                        "total": result_data["total_questions"],
                        "correct": result_data["correct_answers"]
                    })

            # Write mode summaries
            f.write("æ¨¡å¼å¯¹æ¯”:\n")
            f.write("-" * 60 + "\n")

            mode_descriptions = {
                "mode1": "Mode 1: ä»…èƒŒæ™¯",
                "mode2": "Mode 2: èƒŒæ™¯+å½“å‰å¯¹è¯(æˆªæ–­)",
                "mode3": "Mode 3: èƒŒæ™¯+å½“å‰å¯¹è¯(æˆªæ–­)+å‰3ä¸ªå¯¹è¯"
            }

            for mode_name, stats in mode_stats.items():
                if stats["total"] > 0:
                    overall_accuracy = stats["correct"] / stats["total"]
                    mode_display = mode_descriptions.get(mode_name, mode_name)
                    f.write(f"{mode_display}:\n")
                    f.write(f"  æ€»é—®é¢˜æ•°: {stats['total']}\n")
                    f.write(f"  æ­£ç¡®è¯†åˆ«æ•°: {stats['correct']}\n")
                    f.write(f"  æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy:.4f} ({overall_accuracy:.2%})\n")
                    f.write("\n")

            # Detailed breakdown by file
            f.write("\nè¯¦ç»†æ–‡ä»¶ç»“æœ:\n")
            f.write("=" * 80 + "\n")

            for mode_name, stats in mode_stats.items():
                if stats["files"]:
                    mode_display = mode_descriptions.get(mode_name, mode_name)
                    f.write(f"\n{mode_display}:\n")
                    f.write("-" * 60 + "\n")

                    for file_result in stats["files"]:
                        file_name = file_result["file"].replace("eval_", "").replace(f"_{model_name.replace('-', '_')}.json", "")
                        f.write(f"  {file_name}:\n")
                        f.write(f"    å‡†ç¡®ç‡: {file_result['accuracy']:.4f} ({file_result['accuracy']:.2%})\n")
                        f.write(f"    ç»“æœ: {file_result['correct']}/{file_result['total']}\n")
                        f.write("\n")

        print(f"\næ•´ä½“å‡†ç¡®ç‡æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
    except Exception as e:
        print(f"ä¿å­˜æ•´ä½“å‡†ç¡®ç‡æ‘˜è¦æ—¶å‡ºé”™: {e}")

def main():
    """Main evaluation function for justice keyword identification using Qwen2.5-70B"""

    # Configuration
    input_dir = os.path.join(project_root, "data", "generate_justice_questions")
    output_dir = os.path.join(project_root, "result_mode", "generate_result_justice", "qwen2.5_72b_instruct")
    os.makedirs(output_dir, exist_ok=True)

    delay_between_requests = 0.0  # No delay needed for local model

    # Using Qwen2.5-72B local model
    evaluation_model = "qwen2.5-72b-instruct"

    # Dictionary to store all results for overall summary
    all_results = {}

    # Get all JSON files from input directory
    json_files = [f for f in os.listdir(input_dir) if f.endswith('.json')]

    if not json_files:
        print(f"åœ¨{input_dir}ä¸­æ²¡æœ‰æ‰¾åˆ°JSONæ–‡ä»¶")
        return

    print(f"æ‰¾åˆ°{len(json_files)}ä¸ªè¦è¯„ä¼°çš„JSONæ–‡ä»¶:")
    for f in json_files:
        print(f"  - {f}")

    print(f"\\nå¼€å§‹ä½¿ç”¨{evaluation_model}è¿›è¡Œè¯„ä¼°...")
    print(f"è¾“å…¥ç›®å½•: {input_dir}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ä»»åŠ¡ç±»å‹: æ³•å®˜å…³é”®è¯è¯†åˆ«")
    print(f"ä¸ºæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆ3ä¸ªæ¨¡å¼: èƒŒæ™¯ & èƒŒæ™¯+å½“å‰å¯¹è¯ & èƒŒæ™¯+å‰3ä¸ªå¯¹è¯+å½“å‰å¯¹è¯")
    print("-" * 60)

    # Create single evaluator instance
    print("ğŸš€ Initializing Qwen2.5 evaluator...")
    evaluator = JusticeKeywordEvaluator()
    print("âœ… Qwen2.5 evaluator initialized successfully...")

    # Process each file with TWO different context settings for comparison
    for json_file in json_files:
        print(f"\\n{'='*60}")
        print(f"è¯„ä¼°: {json_file}")
        print(f"{'='*60}")

        input_file = os.path.join(input_dir, json_file)
        base_name = os.path.splitext(json_file)[0]

        # Clean the base name to remove "justice_keywords_" prefix if present
        if base_name.startswith("justice_keywords_"):
            base_name = base_name[18:]  # Remove "justice_keywords_" prefix

        # MODE 1: Background only (no conversations)
        print(f"\\n--- Mode 1: ä»…èƒŒæ™¯ (æ— å¯¹è¯) ---")
        output_file_mode1 = os.path.join(output_dir, f"eval_{base_name}_mode1_background_only_{evaluation_model.replace('-', '_')}.json")

        # Reset results for new evaluation
        evaluator.results = {
            "evaluation_timestamp": datetime.now().isoformat(),
            "total_questions": 0,
            "correct_answers": 0,
            "accuracy": 0.0,
            "detailed_results": [],
            "task_type": "justice_keyword_identification"
        }
        evaluator.dialogue_summaries = {}
        evaluator.context_patterns = {}

        evaluator.evaluate_dataset(
            input_file=input_file,
            output_file=output_file_mode1,
            delay=delay_between_requests,
            model=evaluation_model,
            use_cumulative_context=False,
            analyze_patterns=False,
            include_current_conversation=False
        )

        # Store results for overall summary
        mode1_result_key = f"eval_{base_name}_mode1_background_only_{evaluation_model.replace('-', '_')}.json"
        all_results[mode1_result_key] = {
            "total_questions": evaluator.results["total_questions"],
            "correct_answers": evaluator.results["correct_answers"],
            "accuracy": evaluator.results["accuracy"]
        }

        # MODE 2: Background + Current conversation (truncated)
        print(f"\\n--- Mode 2: èƒŒæ™¯ + å½“å‰å¯¹è¯(æˆªæ–­) ---")
        output_file_mode2 = os.path.join(output_dir, f"eval_{base_name}_mode2_with_current_{evaluation_model.replace('-', '_')}.json")

        # Reset results for new evaluation
        evaluator.results = {
            "evaluation_timestamp": datetime.now().isoformat(),
            "total_questions": 0,
            "correct_answers": 0,
            "accuracy": 0.0,
            "detailed_results": [],
            "task_type": "justice_keyword_identification"
        }
        evaluator.dialogue_summaries = {}
        evaluator.context_patterns = {}

        evaluator.evaluate_dataset(
            input_file=input_file,
            output_file=output_file_mode2,
            delay=delay_between_requests,
            model=evaluation_model,
            use_cumulative_context=False,
            analyze_patterns=False,
            include_current_conversation=True
        )

        # Store results for overall summary
        mode2_result_key = f"eval_{base_name}_mode2_with_current_{evaluation_model.replace('-', '_')}.json"
        all_results[mode2_result_key] = {
            "total_questions": evaluator.results["total_questions"],
            "correct_answers": evaluator.results["correct_answers"],
            "accuracy": evaluator.results["accuracy"]
        }



        # MODE 3: Background + Current conversation (truncated) + Previous 3 conversations (full text)
        print(f"\\n--- Mode 3: èƒŒæ™¯ + å½“å‰å¯¹è¯(æˆªæ–­) + å‰3ä¸ªå¯¹è¯(å®Œæ•´) ---")
        output_file_mode3 = os.path.join(output_dir, f"eval_{base_name}_mode3_with_previous_{evaluation_model.replace('-', '_')}.json")

        # Reset results for new evaluation
        evaluator.results = {
            "evaluation_timestamp": datetime.now().isoformat(),
            "total_questions": 0,
            "correct_answers": 0,
            "accuracy": 0.0,
            "detailed_results": [],
            "task_type": "justice_keyword_identification"
        }
        evaluator.dialogue_summaries = {}
        evaluator.context_patterns = {}

        evaluator.evaluate_dataset(
            input_file=input_file,
            output_file=output_file_mode3,
            delay=delay_between_requests,
            model=evaluation_model,
            use_cumulative_context=True,
            analyze_patterns=False,
            include_current_conversation=True
        )

        # Store results for overall summary
        mode3_result_key = f"eval_{base_name}_mode3_with_previous_{evaluation_model.replace('-', '_')}.json"
        all_results[mode3_result_key] = {
            "total_questions": evaluator.results["total_questions"],
            "correct_answers": evaluator.results["correct_answers"],
            "accuracy": evaluator.results["accuracy"]
        }

        print(f"{json_file}çš„è¯„ä¼°å®Œæˆ (ç”Ÿæˆäº†3ä¸ªæ¨¡å¼)")

    print(f"\\n{'='*60}")
    print("æ‰€æœ‰è¯„ä¼°å®Œæˆ")
    print(f"{'='*60}")
    print(f"ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"è¯„ä¼°çš„æ–‡ä»¶: {len(json_files)} ä¸ªæ–‡ä»¶ Ã— æ¯ä¸ª3ä¸ªæ¨¡å¼")

    # Generate overall accuracy summary
    save_overall_accuracy_summary(output_dir, evaluation_model, all_results)

if __name__ == "__main__":
    main()