# Get project root directory
import os
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))))

import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Dict, List, Tuple, Optional
import time
from datetime import datetime
import gc

class IntentionEvaluator:
    def __init__(self, gpu_id: int = 0):
        self.gpu_id = gpu_id
        self.device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.tokenizer = None
        self.init_qwen25_model()
        self.results = {
            "evaluation_timestamp": datetime.now().isoformat(),
            "total_questions": 0,
            "correct_answers": 0,
            "accuracy": 0.0,
            "detailed_results": [],
            "task_type": "intention_analysis"
        }
        self.dialogue_summaries = {}
        self.context_patterns = {}

    def init_qwen25_model(self):
        """Initialize Qwen2.5 70B model and tokenizer"""
        model_path = os.path.join(project_root, "qwen_models", "Qwen_Qwen2.5-72B-Instruct")

        print(f"ğŸš€ GPU {self.gpu_id}: åŠ è½½Qwen2.5-72B-Instructæ¨¡å‹...")
        print(f"ğŸ”§ GPU {self.gpu_id}: ä½¿ç”¨è®¾å¤‡: {self.device}")

        try:
            # è®¾ç½®GPUè®¾å¤‡å¹¶æ¸…ç†å†…å­˜
            torch.cuda.set_device(self.gpu_id)
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

            print(f"ğŸ“¥ GPU {self.gpu_id}: åŠ è½½tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                use_fast=False
            )

            print(f"ğŸ“¥ GPU {self.gpu_id}: åŠ è½½æ¨¡å‹åˆ°GPU...")
            try:
                # å°è¯•ä½¿ç”¨flash attention
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    torch_dtype=torch.float16,
                    device_map="auto",  # Auto-distribute across available GPUs for 72B model
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                    attn_implementation="flash_attention_2"
                )
            except Exception:
                print(f"âš ï¸ GPU {self.gpu_id}: Flash Attentionä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤attention")
                # å›é€€åˆ°é»˜è®¤attention
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    torch_dtype=torch.float16,
                    device_map="auto",  # Auto-distribute across available GPUs for 72B model
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )

            if torch.cuda.is_available():
                memory_after = torch.cuda.memory_allocated(self.gpu_id) / 1e9
                print(f"âœ… GPU {self.gpu_id}: Qwen2.5-7B-Instructæ¨¡å‹åŠ è½½å®Œæˆ! å†…å­˜ä½¿ç”¨: {memory_after:.2f}GB")

        except Exception as e:
            print(f"âŒ GPU {self.gpu_id}: æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            torch.cuda.empty_cache()
            raise e

    def count_tokens(self, text: str) -> int:
        """Count tokens in text using the tokenizer"""
        try:
            tokens = self.tokenizer(text, return_tensors="pt", add_special_tokens=False)
            return tokens.input_ids.shape[1]
        except:
            # Fallback: rough estimation
            return len(text) // 4

    def reduce_conversations_to_fit_tokens(self, context: str, max_tokens: int, use_cumulative_context: bool = True) -> str:
        """Reduce number of conversations instead of hard truncation to fit token limit"""
        current_tokens = self.count_tokens(context)

        if current_tokens <= max_tokens:
            return context

        print(f"âš ï¸ GPU {self.gpu_id}: Context too long ({current_tokens} tokens), reducing conversations to fit {max_tokens} tokens")

        # If no previous conversations, fall back to smart truncation
        if "Previous Conversations" not in context or not use_cumulative_context:
            return self.truncate_context_smartly(context, max_tokens)

        # Split context into background and conversation parts
        parts = context.split("\\n\\nPrevious Conversations")
        base_part = parts[0]  # Background context
        base_tokens = self.count_tokens(base_part)

        # If base part is already too long, truncate it first
        if base_tokens > max_tokens - 500:  # Reserve 500 tokens for at least some conversation
            print(f"    âš ï¸ Background context itself is too long ({base_tokens} tokens), truncating background first")
            base_part = self.truncate_context_smartly(base_part, max_tokens - 500)
            base_tokens = self.count_tokens(base_part)

        if len(parts) > 1:
            # We have previous conversations, reduce them gradually
            remaining_tokens = max_tokens - base_tokens - 200  # Reserve for formatting

            # Parse conversation section
            prev_conv_section = parts[1]
            conversations_text = prev_conv_section.split("\\n\\n---\\n\\n")

            # Start with most recent conversations and add until we hit limit
            conversations_to_keep = []
            current_conv_tokens = 0

            # conversations_text[0] contains the header, conversations_text[1:] contain actual conversations
            if len(conversations_text) > 1:
                for conv_text in reversed(conversations_text[1:]):  # Most recent first
                    conv_tokens = self.count_tokens(conv_text)
                    if current_conv_tokens + conv_tokens <= remaining_tokens:
                        conversations_to_keep.insert(0, conv_text)  # Insert at beginning for chronological order
                        current_conv_tokens += conv_tokens
                        print(f"    âœ“ Keeping conversation ({conv_tokens} tokens, total: {current_conv_tokens})")
                    else:
                        print(f"    âœ— Dropping conversation ({conv_tokens} tokens, would exceed limit)")

            # Rebuild context with reduced conversations
            if conversations_to_keep:
                result = base_part + f"\\n\\nPrevious Conversations (Last {len(conversations_to_keep)}, reduced to fit tokens):\\n" + "\\n\\n---\\n\\n".join(conversations_to_keep)
            else:
                result = base_part  # No previous conversations fit, use only background
                print(f"    âš ï¸ No previous conversations fit in token budget, using background only")
        else:
            result = base_part

        final_tokens = self.count_tokens(result)
        print(f"âœ‚ï¸ GPU {self.gpu_id}: Conversation reduction complete: {current_tokens} -> {final_tokens} tokens")
        return result

    def truncate_context_smartly(self, context: str, max_tokens: int) -> str:
        """Intelligently truncate context to fit within token limit - used as fallback when conversation reduction is not applicable"""
        current_tokens = self.count_tokens(context)

        if current_tokens <= max_tokens:
            return context

        print(f"âš ï¸ GPU {self.gpu_id}: Applying fallback smart truncation ({current_tokens} tokens) -> {max_tokens} tokens")

        # Split by sections and keep the most important parts
        sections = context.split('\n\n')

        # Always keep background if it exists
        background_parts = []
        cumulative_parts = []
        current_parts = []

        for section in sections:
            if 'Background Context:' in section or 'Case Background' in section:
                background_parts.append(section)
            elif 'Previous Conversations' in section or 'Previous Dialogue' in section:
                cumulative_parts.append(section)
            elif 'Current Conversation' in section:
                current_parts.append(section)
            else:
                background_parts.append(section)

        # Prioritize: Current > Background > Previous (most recent first)
        final_parts = []
        tokens_used = 0

        # Add current conversation context first (most important)
        for part in current_parts:
            part_tokens = self.count_tokens(part)
            if tokens_used + part_tokens <= max_tokens:
                final_parts.append(part)
                tokens_used += part_tokens

        # Add background context
        for part in background_parts:
            part_tokens = self.count_tokens(part)
            if tokens_used + part_tokens <= max_tokens:
                final_parts.append(part)
                tokens_used += part_tokens
            else:
                # Truncate this part
                remaining_tokens = max_tokens - tokens_used
                if remaining_tokens > 100:  # Only add if meaningful space left
                    truncated_part = part[:remaining_tokens * 4]  # Rough char to token ratio
                    actual_tokens = self.count_tokens(truncated_part)
                    if tokens_used + actual_tokens <= max_tokens:
                        final_parts.append(truncated_part + "...")
                        tokens_used += actual_tokens
                break

        # Add previous conversations (most recent first, but only if space)
        cumulative_parts.reverse()  # Most recent first
        for part in cumulative_parts:
            part_tokens = self.count_tokens(part)
            if tokens_used + part_tokens <= max_tokens:
                final_parts.append(part)
                tokens_used += part_tokens
            else:
                # Try to fit a truncated version
                remaining_tokens = max_tokens - tokens_used
                if remaining_tokens > 200:  # Only if significant space
                    truncated_part = part[:remaining_tokens * 4]
                    actual_tokens = self.count_tokens(truncated_part)
                    if tokens_used + actual_tokens <= max_tokens:
                        final_parts.append(truncated_part + "...")
                        tokens_used += actual_tokens
                break

        result = '\n\n'.join(final_parts)
        final_tokens = self.count_tokens(result)
        print(f"âœ‚ï¸ GPU {self.gpu_id}: Smart truncation complete: {current_tokens} -> {final_tokens} tokens")
        return result

    def generate_response(self, messages: List[Dict], max_tokens: int = 300, temperature: float = 0.1):
        """ä½¿ç”¨Qwen2.5-7Bæ¨¡å‹ç”Ÿæˆå“åº”"""
        try:
            print(f"ğŸ”§ GPU {self.gpu_id} - å¼€å§‹ç”Ÿæˆï¼Œmax_tokens={max_tokens}, temperature={temperature}")

            # Apply chat template
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            print(f"ğŸ“ GPU {self.gpu_id} - æ„å»ºçš„prompté•¿åº¦: {len(text)} å­—ç¬¦")

            # Use larger context for 72B model (more capable)
            model_max_length = 65536  # Qwen2.5-72B supports larger context
            reserved_tokens_for_generation = max_tokens + 200
            max_input_length = model_max_length - reserved_tokens_for_generation

            # Count actual tokens
            input_token_count = self.count_tokens(text)
            print(f"ğŸ”¢ GPU {self.gpu_id} - è¾“å…¥tokenæ•°: {input_token_count} (maxå…è®¸: {max_input_length})")

            # If input is too long, intelligently truncate
            if input_token_count > max_input_length:
                print(f"âš ï¸ GPU {self.gpu_id}: è¾“å…¥è¿‡é•¿ï¼Œéœ€è¦æ™ºèƒ½æˆªæ–­")
                # Extract user content from messages and truncate it
                for msg in messages:
                    if msg['role'] == 'user' and len(msg['content']) > 1000:
                        msg['content'] = self.truncate_context_smartly(msg['content'], max_input_length - 200)

                # Rebuild text after truncation
                text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                input_token_count = self.count_tokens(text)
                print(f"âœ‚ï¸ GPU {self.gpu_id}: æˆªæ–­åtokenæ•°: {input_token_count}")

            # Tokenize with final length check
            model_inputs = self.tokenizer([text], return_tensors="pt", padding=True, truncation=True, max_length=max_input_length).to(self.device)

            # Ensure attention mask exists
            if model_inputs.attention_mask is None:
                model_inputs.attention_mask = torch.ones_like(model_inputs.input_ids)

            # Final safety check and hard truncation if needed
            actual_input_length = model_inputs.input_ids.shape[1]
            if actual_input_length > max_input_length:
                print(f"âš ï¸ GPU {self.gpu_id}: æœ€ç»ˆç¡¬æˆªæ–­ {actual_input_length} -> {max_input_length} tokens")
                model_inputs.input_ids = model_inputs.input_ids[:, -max_input_length:]
                model_inputs.attention_mask = model_inputs.attention_mask[:, -max_input_length:]
                actual_input_length = max_input_length

            # Calculate safe generation tokens
            max_generation_tokens = min(max_tokens, model_max_length - actual_input_length - 50)

            if max_generation_tokens < 20:
                print(f"âŒ GPU {self.gpu_id}: ç”Ÿæˆç©ºé—´ä¸¥é‡ä¸è¶³ ({max_generation_tokens} tokens)ï¼Œå¼ºåˆ¶è®¾ç½®ä¸º100")
                max_generation_tokens = 100  # Good balance for 7B model

            with torch.no_grad():
                # Clear GPU cache and check memory before inference
                torch.cuda.empty_cache()
                if torch.cuda.is_available():
                    memory_before = torch.cuda.memory_allocated(self.gpu_id) / 1e9
                    print(f"âš¡ GPU {self.gpu_id} - å¼€å§‹æ¨¡å‹æ¨ç†... (å†…å­˜ä½¿ç”¨: {memory_before:.2f}GB)")
                print(f"ğŸ¯ GPU {self.gpu_id} - ä½¿ç”¨ {max_generation_tokens} tokens ç”¨äºç”Ÿæˆ (è¾“å…¥: {actual_input_length}, æ€»é¢„ç®—: {model_max_length})")

                try:
                    generated_ids = self.model.generate(
                        model_inputs.input_ids,
                        attention_mask=model_inputs.attention_mask,
                        max_new_tokens=max_generation_tokens,
                        do_sample=True,
                        temperature=temperature,
                        top_p=0.8,
                        pad_token_id=self.tokenizer.eos_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                        use_cache=True,
                        repetition_penalty=1.1
                    )
                    print(f"âš¡ GPU {self.gpu_id} - æ¨¡å‹æ¨ç†å®Œæˆ")
                except RuntimeError as e:
                    if "out of memory" in str(e).lower():
                        print(f"âŒ GPU {self.gpu_id} - GPU OOM é”™è¯¯ï¼Œå°è¯•æ›´ä¿å®ˆçš„è®¾ç½®")
                        torch.cuda.empty_cache()
                        # Try with smaller generation
                        max_generation_tokens = min(50, max_generation_tokens // 2)
                        generated_ids = self.model.generate(
                            model_inputs.input_ids,
                            attention_mask=model_inputs.attention_mask,
                            max_new_tokens=max_generation_tokens,
                            do_sample=False,  # Greedy decoding to save memory
                            pad_token_id=self.tokenizer.eos_token_id,
                            eos_token_id=self.tokenizer.eos_token_id,
                            use_cache=False
                        )
                        print(f"âš¡ GPU {self.gpu_id} - ä½¿ç”¨ä¿å®ˆè®¾ç½®å®Œæˆæ¨ç† ({max_generation_tokens} tokens)")
                    else:
                        raise e

            print(f"ğŸ”¢ GPU {self.gpu_id} - ç”Ÿæˆçš„tokenæ•°: {generated_ids.shape[1]}")
            print(f"ğŸ”¢ GPU {self.gpu_id} - æ–°ç”Ÿæˆçš„tokenæ•°: {generated_ids.shape[1] - model_inputs.input_ids.shape[1]}")

            response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            print(f"ğŸ“„ GPU {self.gpu_id} - å®Œæ•´responseé•¿åº¦: {len(response)}")

            # æ›´å¥½çš„æå–æ–¹æ³•ï¼šåªè§£ç æ–°ç”Ÿæˆçš„token
            new_tokens = generated_ids[0][model_inputs.input_ids.shape[1]:]
            ai_response = self.tokenizer.decode(new_tokens, skip_special_tokens=True).strip()
            print(f"ğŸ“„ GPU {self.gpu_id} - æå–çš„ai_responseé•¿åº¦: {len(ai_response)}")

            print("-" * 50)

            # å¦‚æœç”Ÿæˆä¸ºç©ºï¼Œé¢å¤–è°ƒè¯•ä¿¡æ¯
            if not ai_response:
                print(f"âŒ GPU {self.gpu_id} - ç”Ÿæˆä¸ºç©ºï¼è°ƒè¯•ä¿¡æ¯:")
                print(f"   åŸå§‹prompté•¿åº¦: {len(text)}")
                print(f"   å®Œæ•´response: {repr(response)}")
                print(f"   æ˜¯å¦å®Œæ•´responseç­‰äºprompt: {response == text}")
                print(f"   å®Œæ•´responseå100å­—ç¬¦: {repr(response[-100:])}")

            # æ¸…ç†ä¸´æ—¶å¼ é‡
            del model_inputs, generated_ids
            torch.cuda.empty_cache()

            return ai_response

        except Exception as e:
            print(f"âŒ GPU {self.gpu_id} - ç”Ÿæˆå¼‚å¸¸: {str(e)}")
            import traceback
            traceback.print_exc()
            torch.cuda.empty_cache()
            return f"Error generating response on GPU {self.gpu_id}: {str(e)}"

    def get_qwen25_answer(self, messages: List[Dict], temperature: float = 0.1) -> str:
        """Get Qwen2.5's answer to the question"""
        try:
            response = self.generate_response(messages, max_tokens=2048, temperature=temperature)
            answer = response.strip().upper()
            import re
            print(f"ğŸ” Raw answer from model: {repr(response)}")

            # Extract just the letter if there's extra text
            if len(answer) > 1:
                """
                ä» LLM è¾“å‡ºé‡Œæå– \boxed{...} æ ¼å¼çš„ç­”æ¡ˆ
                """
                match = re.search(r"\\boxed\{(.*?)\}", response)
                if match:
                    return match.group(1).strip()

                # Look for "Answer: X" or "**Answer:** X" patterns first
                answer_pattern_match = re.search(r'[*\s]*answer[*\s]*:?\s*([ABCD])', response, re.IGNORECASE)
                if answer_pattern_match:
                    extracted_letter = answer_pattern_match.group(1).upper()
                    print(f"âœ… Extracted from 'Answer:' pattern: {extracted_letter}")
                    return extracted_letter

                # If not in answer pattern, extract first standalone letter
                first_letter_match = re.search(r'\b([ABCD])\b', response)
                if first_letter_match:
                    extracted_letter = first_letter_match.group(1)
                    print(f"âœ… Extracted standalone letter: {extracted_letter}")
                    return extracted_letter

            # If exact match
            if answer in ['A', 'B', 'C', 'D']:
                print(f"âœ… Direct match answer: {answer}")
                return answer

            print(f"âŒ No valid answer found in: {repr(answer)}")
            return 'INVALID'

        except Exception as e:
            print(f"Error getting Qwen2.5 response: {e}")
            return 'ERROR'

    def build_evaluation_prompt_no_leak(self, cumulative_context: str, question_data: Dict, target_justice: str = '') -> List[Dict]:
        """
        Build evaluation prompt for judicial intention analysis
        Uses cumulative context to understand judicial intentions
        """
        # Format the options nicely
        options_text = "\n".join([f"{key}: {value}" for key, value in question_data["options"].items()])

        messages = [
            {
                'role': 'system',
                'content': (
                    "You are an expert in legal reasoning and judicial intention analysis. "
                    "You will be given background context and a multiple-choice question "
                    "about understanding judicial intentions and reasoning patterns.\n\n"

                    "Your task is to:\n"
                    "1. Carefully analyze the judicial dialogue and reasoning\n"
                    "2. Consider the judge's legal philosophy and decision-making patterns\n"
                    "3. Evaluate the underlying intentions behind judicial questions and comments\n"
                    "4. Consider what legal principles or concerns are driving the judge's inquiries\n"
                    "5. Use your understanding of legal doctrine and judicial behavior\n"
                    "6. Use the background context to inform your analysis of judicial intentions\n"
                    "7. Select the most likely intention or reasoning from the given options\n\n"

                    "Focus on:\n"
                    "- Judicial reasoning patterns and legal philosophy\n"
                    "- Constitutional, statutory, or precedential concerns\n"
                    "- The judge's apparent legal priorities and interpretive approach\n"
                    "- Theory of mind understanding of judicial decision-making\n"
                    "- The underlying intentions driving judicial questions and comments\n\n"

                    "Respond with ONLY the letter of your chosen answer (A, B, C, or D). "
                    "Do not include any explanation or additional text."

                    "Provide brief one short sentence analysis and answer by selecting the options only"
                    "Please reiterate your answer, with your final answer a single answer of the form \\boxed{{answer}} at the end of your response."

                )
            },
            {
                'role': 'user',
                'content': (
                    f"Context (Case Background + Previous Conversations + Current Conversation Before Target Question):\n{cumulative_context}\n\n"
                    f"Target Justice: {target_justice}\n\n"
                    f"Question: {question_data['question']}\n\n"
                    f"Options:\n{options_text}\n\n"
                    f"What is your answer?"
                )
            }
        ]
        return messages

    def evaluate_dataset(self, input_file: str, output_file: str = None, delay: float = 1.0,
                        model: str = "qwen2.5-7b-instruct", use_cumulative_context: bool = True,
                        analyze_patterns: bool = True, include_current_conversation: bool = True,
                        use_summary: bool = False):
        """
        Evaluate the entire prediction dataset with cumulative context
        NO ANSWER LEAKAGE: Only shows context before target question
        """

        # Load the generated questions
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as e:
            print(f"Error loading input file: {e}")
            return

        base_background = data.get('background', '')
        conversations = data.get('conversations', [])

        print(f"å¼€å§‹ä½¿ç”¨{model}è¯„ä¼°{len(conversations)}ä¸ªå¯¹è¯...")
        print(f"åŸºç¡€èƒŒæ™¯é•¿åº¦: {len(base_background)} å­—ç¬¦")
        print(f"ä½¿ç”¨ç´¯ç§¯ä¸Šä¸‹æ–‡: {use_cumulative_context}")
        print(f"åŒ…å«å½“å‰å¯¹è¯: {include_current_conversation}")
        print(f"ä½¿ç”¨æ‘˜è¦å‹ç¼©: {use_summary}")
        print(f"ä»»åŠ¡ç±»å‹: ä¸‹ä¸€ä¸ªé—®é¢˜é¢„æµ‹ (æ— ç­”æ¡ˆæ³„éœ²)")
        print("-" * 60)

        # Sort conversations by ID to ensure proper order
        conversations.sort(key=lambda x: x.get('conversation_id', 0))

        # Track processed conversations for cumulative context
        processed_conversations = []

        # Process each conversation
        for conv in conversations:
            conv_id = conv.get('conversation_id', 0)
            dialogue = conv.get('dialogue', '')
            target_dialogue = conv.get('target_dialogue', dialogue)
            questions = conv.get('questions', [])

            print(f"\nå¤„ç†å¯¹è¯ {conv_id}:")
            print(f"  é—®é¢˜æ•°: {len(questions)}")

            # Process each question in the conversation
            for q_idx, question_data in enumerate(questions):
                if 'question' not in question_data or 'options' not in question_data:
                    print(f"    è·³è¿‡æ ¼å¼é”™è¯¯çš„é—®é¢˜ {q_idx}")
                    continue

                # Build context with background and cumulative conversations
                context = ""
                if base_background:
                    if len(base_background) > 3000:
                        context = base_background[:3000] + "..."
                    else:
                        context = base_background

                # Add cumulative context with dynamic conversation reduction
                if use_cumulative_context:
                    current_conv_id = conv.get('conversation_id', 0)
                    previous_conversations_all = [c for c in conversations if c.get('conversation_id', 0) < current_conv_id]

                    # Sort by conversation_id to maintain order
                    previous_conversations_all.sort(key=lambda x: x.get('conversation_id', 0))

                    # Dynamically determine conversations to include based on token limits
                    max_context_tokens_before_current = 40000  # Large limit for 72B model
                    temp_context_with_prev = context
                    previous_dialogues_to_include = []

                    # Start with most recent conversations and add them one by one
                    for i in range(min(3, len(previous_conversations_all))):  # Up to 6 conversations for 7B model
                        prev_conv = previous_conversations_all[-(i+1)]
                        prev_full_dialogue = prev_conv.get('target_dialogue', prev_conv.get('dialogue', '')).strip()

                        if prev_full_dialogue:
                            # ä½¿ç”¨æ‘˜è¦å‹ç¼©å¯¹è¯ (å¦‚æœå¯ç”¨)
                            if use_summary:
                                conv_id = prev_conv.get('conversation_id', 0)
                                # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜çš„æ‘˜è¦
                                if conv_id not in self.dialogue_summaries:
                                    print(f"    ğŸ”„ ä¸ºå¯¹è¯ {conv_id} ç”Ÿæˆæ‘˜è¦...")
                                    summary = self.summarize_dialogue(prev_full_dialogue, max_chars=3000)
                                    self.dialogue_summaries[conv_id] = summary
                                    print(f"    ğŸ“ æ‘˜è¦å®Œæˆ: {len(prev_full_dialogue)} -> {len(summary)} å­—ç¬¦")
                                else:
                                    print(f"    ğŸ“‹ ä½¿ç”¨ç¼“å­˜æ‘˜è¦ for å¯¹è¯ {conv_id}")

                                dialogue_to_use = self.dialogue_summaries[conv_id]
                                context_type = "æ‘˜è¦"
                            else:
                                dialogue_to_use = prev_full_dialogue
                                context_type = "å®Œæ•´"

                            test_prev_dialogues = [dialogue_to_use] + previous_dialogues_to_include
                            test_context = temp_context_with_prev + f"\\n\\nä»¥å‰çš„å¯¹è¯ (æœ€å {len(test_prev_dialogues)} ä¸ª):\\n" + "\\n\\n---\\n\\n".join(test_prev_dialogues)

                            test_tokens = self.count_tokens(test_context)
                            if test_tokens <= max_context_tokens_before_current:
                                previous_dialogues_to_include = test_prev_dialogues
                                print(f"    âœ“ æ·»åŠ äº†å¯¹è¯ {prev_conv.get('conversation_id', 0)} ({context_type}, æ€»tokenæ•°: {test_tokens})")
                            else:
                                print(f"    âœ— è·³è¿‡å¯¹è¯ {prev_conv.get('conversation_id', 0)} (ä¼šè¶…è¿‡tokené™åˆ¶: {test_tokens})")
                                break

                    if previous_dialogues_to_include:
                        previous_dialogues_to_include.reverse()
                        summary_note = "æ‘˜è¦å‹ç¼©" if use_summary else "å®Œæ•´"
                        context += f"\\n\\nä»¥å‰çš„å¯¹è¯ (æœ€å {len(previous_dialogues_to_include)} ä¸ª, {summary_note}, åŠ¨æ€é™åˆ¶ä»¥é€‚åº”token):\\n" + "\\n\\n---\\n\\n".join(previous_dialogues_to_include)
                        print(f"    æ·»åŠ äº†ç´¯ç§¯ä¸Šä¸‹æ–‡ ({len(previous_dialogues_to_include)} ä¸ªä»¥å‰çš„å¯¹è¯, {summary_note})")

                # Add current conversation context for intention analysis (NO ANSWER LEAKAGE)
                if include_current_conversation and dialogue:
                    # For motivation analysis, we need the dialogue before the current question
                    # to understand the context that led to the judge's intention
                    # We need to find where the current question appears and cut before it
                    current_question = question_data.get('question', '')

                    if current_question:
                        # Try to find the question in the dialogue to cut before it
                        question_snippet = current_question[:100]  # First 100 chars to find location
                        cutoff_position = dialogue.find(question_snippet)

                        if cutoff_position > 0:
                            safe_dialogue = dialogue[:cutoff_position].strip()
                            if safe_dialogue and current_question not in safe_dialogue:
                                context += f"\\n\\nå½“å‰å¯¹è¯ (é—®é¢˜å‰ä¸Šä¸‹æ–‡):\\n{safe_dialogue}"
                                print(f"    å®‰å…¨: æ·»åŠ äº†å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡ ({len(safe_dialogue)} å­—ç¬¦) - åœ¨ä½ç½®{cutoff_position}åˆ‡æ–­")
                            else:
                                print(f"    å®‰å…¨è­¦å‘Š: åˆ‡æ–­çš„å¯¹è¯å¯èƒ½åŒ…å«ç­”æ¡ˆä¿¡æ¯ï¼Œè·³è¿‡å½“å‰å¯¹è¯")
                        else:
                            print(f"    è­¦å‘Š: æ— æ³•åœ¨å¯¹è¯ä¸­å®šä½å½“å‰é—®é¢˜ï¼Œè·³è¿‡å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡ä»¥é¿å…ç­”æ¡ˆæ³„éœ²")
                    else:
                        print(f"    è­¦å‘Š: å½“å‰é—®é¢˜ä¸ºç©ºï¼Œè·³è¿‡å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡")
                elif not include_current_conversation:
                    print(f"    è·³è¿‡: include_current_conversation=Falseï¼Œä¸æ·»åŠ å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡")

                # Final context validation
                context_tokens = self.count_tokens(context)
                max_allowed_context_tokens = 50000  # Large limit for 72B model

                if context_tokens > max_allowed_context_tokens:
                    print(f"    âš ï¸ ä¸Šä¸‹æ–‡è¿‡é•¿ ({context_tokens} tokens)ï¼Œåº”ç”¨å¯¹è¯å‡å°‘")
                    context = self.reduce_conversations_to_fit_tokens(context, max_allowed_context_tokens, use_cumulative_context)
                    context_tokens = self.count_tokens(context)

                context_length = len(context)
                num_context_conversations = min(6, len(processed_conversations)) if use_cumulative_context else 0
                print(f"  é—®é¢˜ {q_idx + 1}/{len(questions)} (å—é™ä¸Šä¸‹æ–‡: {num_context_conversations} ä¸ªä»¥å‰çš„å¯¹è¯, æœ€å¤š6ä¸ª)")
                print(f"    ä¸Šä¸‹æ–‡é•¿åº¦: {context_length} å­—ç¬¦, {context_tokens} tokens")

                # Build evaluation prompt for intention analysis
                messages = self.build_evaluation_prompt_no_leak(context, question_data)

                # Security validation: ensure no answer leakage
                prompt_text = str(messages).lower()
                correct_answer_text = question_data.get('debug_info', {}).get('correct_answer', '')

                # Check if any part of the correct answer appears in the prompt
                if correct_answer_text and correct_answer_text.lower() in prompt_text:
                    print(f"    âš ï¸ å®‰å…¨è­¦å‘Š: åœ¨å¯¹è¯{conv_id}çš„é—®é¢˜{q_idx}ä¸­æ£€æµ‹åˆ°å¯èƒ½çš„ç­”æ¡ˆæ³„éœ²")

                # Additional security checks
                if 'debug_info' in prompt_text or 'correct_answer' in prompt_text:
                    raise ValueError(f"å®‰å…¨è¿è§„: åœ¨å¯¹è¯{conv_id}çš„é—®é¢˜{q_idx}çš„è¯„ä¼°æç¤ºä¸­æ£€æµ‹åˆ°ç­”æ¡ˆæ³„éœ²")

                qwen25_answer = self.get_qwen25_answer(messages)

                # Check if answer is correct
                correct_answer = question_data.get('answer', '').upper()
                is_correct = qwen25_answer == correct_answer

                # Extract target justice from the question text if available
                question_text = question_data.get('question', '')
                target_justice = ''
                if 'Justice' in question_text:
                    import re
                    justice_match = re.search(r'Justice (\w+)', question_text)
                    if justice_match:
                        target_justice = f"Justice {justice_match.group(1)}"

                result = {
                    "conversation_id": conv_id,
                    "question_index": q_idx,
                    "question": question_data['question'],
                    "target_justice": target_justice,
                    "correct_answer": correct_answer,
                    "qwen25_answer": qwen25_answer,
                    "is_correct": is_correct,
                    "options": question_data['options'],
                    "model_used": model,
                    "context_length": context_length,
                    "cumulative_conversations": min(3, len(processed_conversations)) if use_cumulative_context else 0,
                    "task_type": "intention_analysis",
                    "question_index_used": question_data.get('debug_info', {}).get('question_index_in_conversation', 0),
                    "security_check": "no_answer_leakage_verified"
                }

                self.results["detailed_results"].append(result)

                if result['is_correct']:
                    self.results["correct_answers"] += 1
                self.results["total_questions"] += 1

                print(f"    ç»“æœ: {'âœ“' if is_correct else 'âœ—'} (æ¨¡å‹å›ç­”: {qwen25_answer}, æ­£ç¡®ç­”æ¡ˆ: {correct_answer})")

                # Add delay to avoid rate limits
                time.sleep(delay)

            # Add current conversation to processed list AFTER processing all its questions
            processed_conversations.append(conv)

        # Calculate final accuracy
        if self.results["total_questions"] > 0:
            self.results["accuracy"] = self.results["correct_answers"] / self.results["total_questions"]

        # Add configuration info to results
        self.results["model_used"] = model
        self.results["used_cumulative_context"] = use_cumulative_context
        self.results["include_current_conversation"] = include_current_conversation
        self.results["use_summary"] = use_summary
        self.results["analyzed_patterns"] = analyze_patterns
        self.results["dialogue_summaries"] = self.dialogue_summaries
        self.results["context_patterns"] = self.context_patterns

        # Print summary
        self.print_summary()

        # Save results
        if output_file:
            self.save_results(output_file)

    def print_summary(self):
        """Print evaluation summary"""
        print("\n" + "="*60)
        print("é¢„æµ‹è¯„ä¼°æ‘˜è¦ (Qwen2.5-7Bæ¨¡å‹)")
        print("="*60)
        print(f"ä»»åŠ¡ç±»å‹: {self.results.get('task_type', 'æœªçŸ¥')}")
        print(f"ä½¿ç”¨çš„æ¨¡å‹: {self.results.get('model_used', 'æœªçŸ¥')}")
        print(f"ç´¯ç§¯ä¸Šä¸‹æ–‡: {self.results.get('used_cumulative_context', 'æœªçŸ¥')}")
        print(f"åŒ…å«å½“å‰å¯¹è¯: {self.results.get('include_current_conversation', 'æœªçŸ¥')}")
        print(f"ä½¿ç”¨æ‘˜è¦å‹ç¼©: {self.results.get('use_summary', 'æœªçŸ¥')}")
        print(f"æ¨¡å¼åˆ†æ: {self.results.get('analyzed_patterns', 'æœªçŸ¥')}")
        print(f"æ€»é—®é¢˜æ•°: {self.results['total_questions']}")
        print(f"æ­£ç¡®é¢„æµ‹æ•°: {self.results['correct_answers']}")
        print(f"é¢„æµ‹å‡†ç¡®ç‡: {self.results['accuracy']:.2%}")

        # Breakdown by conversation
        conv_stats = {}
        for result in self.results["detailed_results"]:
            conv_id = result["conversation_id"]
            if conv_id not in conv_stats:
                conv_stats[conv_id] = {"correct": 0, "total": 0}
            conv_stats[conv_id]["total"] += 1
            if result["is_correct"]:
                conv_stats[conv_id]["correct"] += 1

        print("\næŒ‰å¯¹è¯åˆ†è§£:")
        for conv_id in sorted(conv_stats.keys()):
            stats = conv_stats[conv_id]
            accuracy = stats["correct"] / stats["total"] if stats["total"] > 0 else 0
            print(f"  å¯¹è¯ {conv_id}: {stats['correct']}/{stats['total']} ({accuracy:.1%})")

        # Context length analysis
        if self.results["detailed_results"]:
            context_lengths = [r.get("context_length", 0) for r in self.results["detailed_results"]]
            avg_context_length = sum(context_lengths) / len(context_lengths)
            print(f"\nå¹³å‡ä¸Šä¸‹æ–‡é•¿åº¦: {avg_context_length:.0f} å­—ç¬¦")

        # Answer distribution analysis
        answer_distribution = {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'ERROR': 0, 'INVALID': 0}
        for result in self.results["detailed_results"]:
            qwen25_answer = result["qwen25_answer"]
            if qwen25_answer in answer_distribution:
                answer_distribution[qwen25_answer] += 1

        print(f"\nQwen2.5-7B ç­”æ¡ˆåˆ†å¸ƒ:")
        for answer, count in answer_distribution.items():
            if count > 0:
                percentage = (count / self.results['total_questions']) * 100
                print(f"  {answer}: {count} ({percentage:.1f}%)")

    def save_results(self, output_file: str):
        """Save evaluation results to JSON file"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False)
            print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            print(f"ä¿å­˜ç»“æœæ—¶å‡ºé”™: {e}")


def save_overall_accuracy_summary(output_dir: str, model_name: str, all_results: dict):
    """Save overall accuracy summary for all files and modes"""
    try:
        print(f"ğŸ”„ ç”Ÿæˆæ‘˜è¦: {model_name}")
        print(f"ğŸ“Š all_resultsåŒ…å« {len(all_results)} ä¸ªæ¡ç›®")
        print(f"ğŸ“ all_results keys: {list(all_results.keys())}")

        summary_file = os.path.join(output_dir, f"{model_name}_overall_accuracy_summary.txt")

        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write(f"æ•´ä½“é¢„æµ‹è¯„ä¼°å‡†ç¡®ç‡æ‘˜è¦ ({model_name.upper()}æ¨¡å‹)\n")
            f.write("=" * 80 + "\n")
            f.write(f"è¯„ä¼°æ—¶é—´: {datetime.now().isoformat()}\n")
            f.write(f"è¯„ä¼°æ–‡ä»¶æ•°: {len(all_results)//2 if len(all_results) > 0 else 0}\n")  # æ¯ä¸ªæ–‡ä»¶æœ‰2ä¸ªç‰ˆæœ¬
            f.write("\n")

            if not all_results:
                f.write("âš ï¸ è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°è¯„ä¼°ç»“æœæ•°æ®\n")
                f.write("å¯èƒ½çš„åŸå› :\n")
                f.write("  - è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯\n")
                f.write("  - æ²¡æœ‰æˆåŠŸå®Œæˆè¯„ä¼°\n")
                f.write("  - æ•°æ®æ ¼å¼ä¸åŒ¹é…\n")
                print(f"âš ï¸ è­¦å‘Š: all_resultsä¸ºç©ºï¼Œç”Ÿæˆäº†åŸºæœ¬æ‘˜è¦")
                return

            # Calculate overall statistics for each mode
            mode_stats = {
                "background_only": {"total": 0, "correct": 0, "files": []},
                "with_current": {"total": 0, "correct": 0, "files": []},
                "with_context": {"total": 0, "correct": 0, "files": []}
            }

            for result_key, result_data in all_results.items():
                print(f"ğŸ“Š å¤„ç†ç»“æœ: {result_key}")
                if "background_only" in result_key:
                    mode_stats["background_only"]["total"] += result_data["total_questions"]
                    mode_stats["background_only"]["correct"] += result_data["correct_answers"]
                    mode_stats["background_only"]["files"].append({
                        "file": result_key,
                        "accuracy": result_data["accuracy"],
                        "total": result_data["total_questions"],
                        "correct": result_data["correct_answers"]
                    })
                elif "with_current" in result_key:
                    mode_stats["with_current"]["total"] += result_data["total_questions"]
                    mode_stats["with_current"]["correct"] += result_data["correct_answers"]
                    mode_stats["with_current"]["files"].append({
                        "file": result_key,
                        "accuracy": result_data["accuracy"],
                        "total": result_data["total_questions"],
                        "correct": result_data["correct_answers"]
                    })
                elif "with_context" in result_key:
                    mode_stats["with_context"]["total"] += result_data["total_questions"]
                    mode_stats["with_context"]["correct"] += result_data["correct_answers"]
                    mode_stats["with_context"]["files"].append({
                        "file": result_key,
                        "accuracy": result_data["accuracy"],
                        "total": result_data["total_questions"],
                        "correct": result_data["correct_answers"]
                    })

            # Write mode summaries
            f.write("æ¨¡å¼å¯¹æ¯”:\n")
            f.write("-" * 60 + "\n")

            for mode_name, stats in mode_stats.items():
                if stats["total"] > 0:
                    overall_accuracy = stats["correct"] / stats["total"]
                    if mode_name == "background_only":
                        mode_display = "ç‰ˆæœ¬1: ä»…èƒŒæ™¯"
                    elif mode_name == "with_current":
                        mode_display = "ç‰ˆæœ¬2: èƒŒæ™¯+å½“å‰å¯¹è¯"
                    else:
                        mode_display = "ç‰ˆæœ¬3: èƒŒæ™¯+å‰3ä¸ªå¯¹è¯+å½“å‰å¯¹è¯"
                    f.write(f"{mode_display}:\n")
                    f.write(f"  æ€»é—®é¢˜æ•°: {stats['total']}\n")
                    f.write(f"  æ­£ç¡®é¢„æµ‹æ•°: {stats['correct']}\n")
                    f.write(f"  æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy:.4f} ({overall_accuracy:.2%})\n")
                    f.write("\n")

            # Detailed breakdown by file
            f.write("\nè¯¦ç»†æ–‡ä»¶ç»“æœ:\n")
            f.write("=" * 80 + "\n")

            for mode_name, stats in mode_stats.items():
                if stats["files"]:
                    if mode_name == "background_only":
                        mode_display = "ç‰ˆæœ¬1: ä»…èƒŒæ™¯"
                    elif mode_name == "with_current":
                        mode_display = "ç‰ˆæœ¬2: èƒŒæ™¯+å½“å‰å¯¹è¯"
                    else:
                        mode_display = "ç‰ˆæœ¬3: èƒŒæ™¯+å‰3ä¸ªå¯¹è¯+å½“å‰å¯¹è¯"
                    f.write(f"\n{mode_display}:\n")
                    f.write("-" * 60 + "\n")

                    for file_result in stats["files"]:
                        file_name = file_result["file"].replace("eval_", "").replace(f"_{model_name.replace('-', '_')}.json", "")
                        f.write(f"  {file_name}:\n")
                        f.write(f"    å‡†ç¡®ç‡: {file_result['accuracy']:.4f} ({file_result['accuracy']:.2%})\n")
                        f.write(f"    ç»“æœ: {file_result['correct']}/{file_result['total']}\n")
                        f.write("\n")

        print(f"âœ… æ•´ä½“å‡†ç¡®ç‡æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ•´ä½“å‡†ç¡®ç‡æ‘˜è¦æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

        # åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„é”™è¯¯æ‘˜è¦æ–‡ä»¶
        try:
            error_summary_file = os.path.join(output_dir, f"{model_name}_overall_accuracy_summary.txt")
            with open(error_summary_file, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write(f"æ•´ä½“é¢„æµ‹è¯„ä¼°å‡†ç¡®ç‡æ‘˜è¦ ({model_name.upper()}æ¨¡å‹)\n")
                f.write("=" * 80 + "\n")
                f.write(f"è¯„ä¼°æ—¶é—´: {datetime.now().isoformat()}\n")
                f.write(f"çŠ¶æ€: é”™è¯¯ - {str(e)}\n")
                f.write("\n")
                f.write("âŒ æ‘˜è¦ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯\n")
                f.write(f"é”™è¯¯è¯¦æƒ…: {str(e)}\n")
                f.write("è¯·æ£€æŸ¥è¯„ä¼°è„šæœ¬å’Œæ•°æ®æ ¼å¼\n")
        except:
            pass
def main():
    """Main evaluation function for intention task using Qwen2.5-72B model"""

    # Configuration
    input_dir = os.path.join(project_root, "data", "output_motivation")
    output_dir = os.path.join(project_root, "result_mode", "intention_result", "qwen2.5_72b_instruct")
    os.makedirs(output_dir, exist_ok=True)

    delay_between_requests = 1.0  # seconds

    # Using Qwen2.5-72B-Instruct model
    evaluation_model = "qwen2.5-72b-instruct"

    # Dictionary to store all results for overall summary
    all_results = {}

    # Get all JSON files from input directory
    json_files = [f for f in os.listdir(input_dir) if f.endswith('.json')]

    if not json_files:
        print(f"åœ¨{input_dir}ä¸­æ²¡æœ‰æ‰¾åˆ°JSONæ–‡ä»¶")
        return

    print(f"æ‰¾åˆ°{len(json_files)}ä¸ªè¦è¯„ä¼°çš„JSONæ–‡ä»¶:")
    for f in json_files:
        print(f"  - {f}")

    print(f"\\nå¼€å§‹ä½¿ç”¨{evaluation_model}è¿›è¡Œè¯„ä¼°...")
    print(f"è¾“å…¥ç›®å½•: {input_dir}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ä¸ºæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆ2ä¸ªç‰ˆæœ¬: ä»…èƒŒæ™¯ & å¸¦ä¸Šä¸‹æ–‡ (é™åˆ¶ä¸ºæœ€å6ä¸ªå¯¹è¯)")
    print("-" * 60)

    # Create single evaluator instance to reuse the loaded model
    print("ğŸš€ ä¸€æ¬¡æ€§åŠ è½½Qwen2.5-72Bæ¨¡å‹ç”¨äºæ‰€æœ‰è¯„ä¼°...")
    evaluator = IntentionEvaluator()
    print("âœ… 72Bæ¨¡å‹åŠ è½½æˆåŠŸï¼Œç»§ç»­è¿›è¡Œè¯„ä¼°...")

    # Process each file with TWO different context settings for comparison
    for json_file in json_files:
        print(f"\\n{'='*60}")
        print(f"è¯„ä¼°: {json_file}")
        print(f"{'='*60}")

        input_file = os.path.join(input_dir, json_file)
        base_name = os.path.splitext(json_file)[0]

        # VERSION 1: Background + Question (no cumulative context, no current conversation)
        print(f"\\n--- ç‰ˆæœ¬ 1: èƒŒæ™¯ + é—®é¢˜ (æ— ç´¯ç§¯ä¸Šä¸‹æ–‡, æ— å½“å‰å¯¹è¯) ---")
        output_file_bg = os.path.join(output_dir, f"eval_{base_name}_background_only_{evaluation_model.replace('-', '_')}.json")

        # Check if file already exists
        if os.path.exists(output_file_bg):
            print(f"â­ï¸  ç‰ˆæœ¬1æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡è¯„ä¼°: {os.path.basename(output_file_bg)}")
            try:
                with open(output_file_bg, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    bg_result_key = f"eval_{base_name}_background_only_{evaluation_model.replace('-', '_')}.json"
                    all_results[bg_result_key] = {
                        "total_questions": existing_data.get("total_questions", 0),
                        "correct_answers": existing_data.get("correct_answers", 0),
                        "accuracy": existing_data.get("accuracy", 0.0)
                    }
                    print(f"âœ… å·²åŠ è½½ç°æœ‰ç»“æœ: {existing_data.get('total_questions', 0)}é¢˜, å‡†ç¡®ç‡{existing_data.get('accuracy', 0.0):.2%}")
            except Exception as e:
                print(f"âš ï¸  åŠ è½½ç°æœ‰æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†é‡æ–°è¯„ä¼°")
        else:
            # Reset results for new evaluation but keep the loaded model
            evaluator.results = {
                "evaluation_timestamp": datetime.now().isoformat(),
                "total_questions": 0,
                "correct_answers": 0,
                "accuracy": 0.0,
                "detailed_results": [],
                "task_type": "intention_analysis"
            }
            evaluator.dialogue_summaries = {}
            evaluator.context_patterns = {}
    
            evaluator.evaluate_dataset(
                input_file=input_file,
                output_file=output_file_bg,
                delay=delay_between_requests,
                model=evaluation_model,
                use_cumulative_context=False,
                analyze_patterns=False,
                include_current_conversation=False
            )
    
            # Store results for overall summary
            bg_result_key = f"eval_{base_name}_background_only_{evaluation_model.replace('-', '_')}.json"
            all_results[bg_result_key] = {
                "total_questions": evaluator.results["total_questions"],
                "correct_answers": evaluator.results["correct_answers"],
                "accuracy": evaluator.results["accuracy"]
            }

        # VERSION 2: Background + current conversation (no cumulative context)
        print(f"\\n--- ç‰ˆæœ¬ 2: èƒŒæ™¯ + å½“å‰å¯¹è¯ (æ— ç´¯ç§¯ä¸Šä¸‹æ–‡) ---")
        output_file_current = os.path.join(output_dir, f"eval_{base_name}_with_current_{evaluation_model.replace('-', '_')}.json")

        # Check if file already exists
        if os.path.exists(output_file_current):
            print(f"â­ï¸  ç‰ˆæœ¬2æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡è¯„ä¼°: {os.path.basename(output_file_current)}")
            try:
                with open(output_file_current, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    current_result_key = f"eval_{base_name}_with_current_{evaluation_model.replace('-', '_')}.json"
                    all_results[current_result_key] = {
                        "total_questions": existing_data.get("total_questions", 0),
                        "correct_answers": existing_data.get("correct_answers", 0),
                        "accuracy": existing_data.get("accuracy", 0.0)
                    }
                    print(f"âœ… å·²åŠ è½½ç°æœ‰ç»“æœ: {existing_data.get('total_questions', 0)}é¢˜, å‡†ç¡®ç‡{existing_data.get('accuracy', 0.0):.2%}")
            except Exception as e:
                print(f"âš ï¸  åŠ è½½ç°æœ‰æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†é‡æ–°è¯„ä¼°")
        else:
            # Reset results for new evaluation but keep the loaded model
            evaluator.results = {
                "evaluation_timestamp": datetime.now().isoformat(),
                "total_questions": 0,
                "correct_answers": 0,
                "accuracy": 0.0,
                "detailed_results": [],
                "task_type": "intention_analysis"
            }
            evaluator.dialogue_summaries = {}
            evaluator.context_patterns = {}
    
            evaluator.evaluate_dataset(
                input_file=input_file,
                output_file=output_file_current,
                delay=delay_between_requests,
                model=evaluation_model,
                use_cumulative_context=False,
                analyze_patterns=False,
                include_current_conversation=True,
                use_summary=False
            )
    
            # Store results for overall summary
            current_result_key = f"eval_{base_name}_with_current_{evaluation_model.replace('-', '_')}.json"
            all_results[current_result_key] = {
                "total_questions": evaluator.results["total_questions"],
                "correct_answers": evaluator.results["correct_answers"],
                "accuracy": evaluator.results["accuracy"]
            }

        # VERSION 3: Background + last 3 conversations + current conversation + question
        print(f"\\n--- ç‰ˆæœ¬ 3: èƒŒæ™¯ + å‰é¢3ä¸ªå¯¹è¯ + å½“å‰å¯¹è¯ + é—®é¢˜ ---")
        output_file_full = os.path.join(output_dir, f"eval_{base_name}_with_context_{evaluation_model.replace('-', '_')}.json")

        # Check if file already exists
        if os.path.exists(output_file_full):
            print(f"â­ï¸  ç‰ˆæœ¬3æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡è¯„ä¼°: {os.path.basename(output_file_full)}")
            try:
                with open(output_file_full, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    full_result_key = f"eval_{base_name}_with_context_{evaluation_model.replace('-', '_')}.json"
                    all_results[full_result_key] = {
                        "total_questions": existing_data.get("total_questions", 0),
                        "correct_answers": existing_data.get("correct_answers", 0),
                        "accuracy": existing_data.get("accuracy", 0.0)
                    }
                    print(f"âœ… å·²åŠ è½½ç°æœ‰ç»“æœ: {existing_data.get('total_questions', 0)}é¢˜, å‡†ç¡®ç‡{existing_data.get('accuracy', 0.0):.2%}")
            except Exception as e:
                print(f"âš ï¸  åŠ è½½ç°æœ‰æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†é‡æ–°è¯„ä¼°")
        else:
            # Reset results for new evaluation but keep the loaded model
            evaluator.results = {
                "evaluation_timestamp": datetime.now().isoformat(),
                "total_questions": 0,
                "correct_answers": 0,
                "accuracy": 0.0,
                "detailed_results": [],
                "task_type": "intention_analysis"
            }
            evaluator.dialogue_summaries = {}
            evaluator.context_patterns = {}
    
            evaluator.evaluate_dataset(
                input_file=input_file,
                output_file=output_file_full,
                delay=delay_between_requests,
                model=evaluation_model,
                use_cumulative_context=True,
                analyze_patterns=False,
                include_current_conversation=True,
                use_summary=False
            )
    
            # Store results for overall summary
            full_result_key = f"eval_{base_name}_with_context_{evaluation_model.replace('-', '_')}.json"
            all_results[full_result_key] = {
                "total_questions": evaluator.results["total_questions"],
                "correct_answers": evaluator.results["correct_answers"],
                "accuracy": evaluator.results["accuracy"]
            }

        print(f"{json_file}çš„è¯„ä¼°å®Œæˆ (ç”Ÿæˆäº†3ä¸ªç‰ˆæœ¬)")

    print(f"\\n{'='*60}")
    print("æ‰€æœ‰è¯„ä¼°å®Œæˆ")
    print(f"{'='*60}")
    print(f"ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"è¯„ä¼°çš„æ–‡ä»¶: {len(json_files)} ä¸ªæ–‡ä»¶ Ã— æ¯ä¸ª3ä¸ªç‰ˆæœ¬")

    # Generate overall accuracy summary
    save_overall_accuracy_summary(output_dir, evaluation_model, all_results)

if __name__ == "__main__":
    main()